---
permalink: /
title: "Biography"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am Wenyong Zhou (周文涌), a PhD graduate from The University of Hong Kong (HKU), where I was mentored by Prof. Ngai Wong and Prof. Can Li in the Next Gen AI Lab. I hold a Bachelor’s degree from Tianjin University (2019), guided by Prof. Yugong Wu, and a Master’s degree in Electrical and Computer Engineering from Northwestern University, where I was mentored by Prof. Seda Ogrenci.

My research focuses on implicit neural representations (INRs) as an efficient data paradigm and emerging computational frameworks, including compute-in-memory (CIM) and stochastic computing. Additionally, I am passionate about improving the efficiency of Large Language Models (LLMs) through techniques such as quantization, pruning, and knowledge distillation.

Since November 2024, I have been working at Zhicun (Witmem) Technology, focusing on low-bit training of LLMs, particularly for compatibility with analog CIM hardware. Previously, I enjoyed enriching internship experiences at Bytedance and JD.com in 2021 and 2023, respectively.


Research Interests
======
- Data is the fuel of the AI era, and implicit neural representations (INRs) offer an innovative approach for encoding data such as images, signals, and 3D scenes in a continuous and compact form using neural networks. This reduces the need for extensive storage while maintaining high-fidelity representations.

- Computing power drives modern AI, but classical digital computers face challenges due to the separation of compute and storage units. Compute-in-memory (CIM) integrates memory and computation into a single unit, significantly reducing data movement and making it especially effective for accelerating AI workloads.

- LLMs have become a cornerstone of modern AI, driving advancements in applications ranging from natural language understanding to content generation. Minimizing their size and computational requirements without compromising performance democratizes access to advanced AI capabilities and broadens their range of applications.


Recent News
======
- 2025.08 - One paper was accepted by ASICON 2025.
- 2025.06 - One paper was accepted by IEEE TCAS-II.
- 2025.03 - One paper was accepted by ICME 2025.
- 2025.01 - One paper was accepted by EDTM 2025.
- 2024.12 - Two papers were accepted by ICCASP 2025.
- 2024.11 - One paper was accepted by DATE 2025.

Selected Publications 
======
*: Equal contribution.
- INRs
  - **W. Zhou\***, B. Li\*, T. Wu, C. Ding, Z. Liu and N. Wong. QuadINR: Quadratic Implicit Neural Representations for Efficient Memristor-based CIM System, *IEEE Transactions on Circuits and Systems II: Express Briefs*.  
  - J. Ren\*, **W. Zhou\***, T. Wu, Y. Cheng, Z. Liu and N. Wong. Patch-Based Implicit Neural Representations for Efficient and Scalable Inference, *IEEE Transactions on Circuits and Systems II: Express Briefs*, (Major revision).
  - **W. Zhou\***, J. Ren\*, T. Wu, Y. Cheng, Z. Liu and N. Wong. Distribution-Aware Hadamard Quantization for Hardware-Efficient Implicit Neural Representations, *2025 IEEE International Conference on Multimedia and Expo (ICME), Nantes, France, 2025*.
  - **W. Zhou\***, T. Wu\*, Y. Cheng, C. Zhang, Z. Liu and N. Wong. MINR: Efficient Implicit Neural Representations for Multi-Image Encoding, *ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Hyderabad, India, 2025*.
  - **W. Zhou\***, Y. Cheng\*, T. Wu, C. Zhang, Z. Liu and N. Wong. Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations, *ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Hyderabad, India, 2025.
  
- Next-Generation Computing Platforms
  - **W. Zhou**, Y. Ren, Z. Liu and N. Wong. Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators, *IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems*.
  - T. Wu\*, C. Ding\*, **W. Zhou\***, Y. Cheng, X. Feng, C. Shi, Z. Liu and N. Wong. HaLoRA: Hardware-aware Low-Rank Adaptation for Large Language Models Based on Hybrid Compute-in-Memory Architecture, *IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems*, (under review).
  - **W. Zhou**, T. Wu, C. Ding, Y. Ren, Z. Liu and N. Wong. Towards RRAM-based Transformer-based Vision Models with Noise-aware Knowledge Distillation, *2025 Design, Automation & Test in Europe Conference & Exhibition (DATE), Lyon, France, 2025*.
  - **W. Zhou\***, Y. Ren\*, J. Zhou, C. Ding, Z. Liu, and N. Wong, RRAM-Based Isotropic CNNs with High Robustness and Resource Utilization Rate, *2025 9th IEEE Electron Devices Technology & Manufacturing Conference (EDTM), Hong Kong, China, 2025*.
  - **W. Zhou\***, Y. Ren\*, J. Zhou, T. Hou, and N. Wong, A Time- and Energy-Efficient CNN with Dense Connections on Memristor-Based Chips, *2023 IEEE 15th International Conference on ASIC (ASICON), Nanjing, China, 2023*.

- Efficient LLMs
  - On the way

More about me
------
- I enjoy staying active and challenging myself through sports like table tennis, badminton, and basketball. These activities not only keep me physically fit but also teach me the value of teamwork, strategy, and perseverance.
- I am deeply passionate about reading, particularly delving into history and politics through a financial lens. I find it fascinating to uncover how financial systems and economic decisions have shaped societies and historical events.
- Traveling is one of my greatest joys. It allows me to explore new cultures, meet diverse people, and experience the world from different perspectives. Each trip enriches my understanding of humanity and inspires new ways of thinking.

(Last updated on Aug., 2025)
