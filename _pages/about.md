---
permalink: /
title: "Biography"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am Wenyong Zhou (周文涌), and I earned my PhD from The University of Hong Kong (HKU), where I had the privilege of being guided by the inspiring mentorship of Prof. Ngai Wong and Prof. Can Li as a proud member of the vibrant Next Gen AI (NGai) Lab family. My academic journey began with a Bachelor’s degree in 2019 from the School of Microelectronics at Tianjin University, where I was fortunate to work under the guidance of Prof. Yugong Wu, whose support sparked my curiosity for research. I later pursued my Master’s degree in Electrical and Computer Engineering at the McCormick School of Engineering, Northwestern University, where I was deeply influenced by the visionary mentorship of Prof. Seda Ogrenci.

My research focuses on implicit neural representations (INRs), exploring their potential for efficient data representation and processing. I am also working on developing next-generation computing platforms, with a particular focus on compute-in-memory (CIM) technologies to enhance efficiency. Additionally, I am passionate about optimizing Large Language Models (LLMs) through techniques such as quantization, pruning, and knowledge distillation to improve their performance and scalability.

Currently, I am working at Zhicun (Witmem) Technology, starting from November 2024, focusing on low-bit training of LLMs using cutting-edge analog compute-in-memory (CIM) technology. This research aims to unlock the potential of low-bit training, enabling more energy-efficient, faster, and scalable LLM deployment on advanced hardware.


Research Interests
======
- INRs
  - INR are a novel approach for encoding data, such as images, signals, and 3D scenes, in a continuous and compact form using neural networks.
  - My research focuses on improving the memory efficiency and generalization capabilities of these representations.
  - I am working on techniques to accelerate their training and inference for real-world deployment.
####################################
- Next-Generation Computing Platforms
  - Next-generation computing platforms aim to overcome the limitations of traditional hardware architectures, and analog CIM technology is at the forefront of this innovation.
  - My research involves designing and optimizing CIM hardware to accelerate AI computations while improving energy efficiency. 
  - I also address challenges related to device reliability, precision, and scalability, ensuring these platforms are practical for real-world applications.
####################################
- Efficient LLMs
  - Optimizing LLMs for efficiency without sacrificing performance is critical as these models grow in size.
  - My research focuses on developing advanced techniques such as quantization, pruning, and knowledge distillation to reduce the size and computational requirements of LLMs.
  - I aim to enable their deployment on resource-constrained devices while maintaining high accuracy and exploring methods to make their training and inference more energy-efficient and scalable.

Selected Publications (*represents equal contribution)
======
- INRs
  - INR are a novel approach for encoding data, such as images, signals, and 3D scenes, in a continuous and compact form using neural networks.
  - My research focuses on improving the memory efficiency and generalization capabilities of these representations.
  - I am working on techniques to accelerate their training and inference for real-world deployment.
####################################
- Next-Generation Computing Platforms
  - Next-generation computing platforms aim to overcome the limitations of traditional hardware architectures, and analog CIM technology is at the forefront of this innovation.
  - My research involves designing and optimizing CIM hardware to accelerate AI computations while improving energy efficiency. 
  - I also address challenges related to device reliability, precision, and scalability, ensuring these platforms are practical for real-world applications.
####################################
- Efficient LLMs
  - Optimizing LLMs for efficiency without sacrificing performance is critical as these models grow in size.
  - My research focuses on developing advanced techniques such as quantization, pruning, and knowledge distillation to reduce the size and computational requirements of LLMs.
  - I aim to enable their deployment on resource-constrained devices while maintaining high accuracy and exploring methods to make their training and inference more energy-efficient and scalable.

More about me
------
- I enjoy staying active and challenging myself through sports like table tennis, badminton, and basketball. These activities not only keep me physically fit but also teach me the value of teamwork, strategy, and perseverance.
- I am deeply passionate about reading, particularly delving into history and politics through a financial lens. I find it fascinating to uncover how financial systems and economic decisions have shaped societies and historical events.
- Traveling is one of my greatest joys. It allows me to explore new cultures, meet diverse people, and experience the world from different perspectives. Each trip enriches my understanding of humanity and inspires new ways of thinking.


