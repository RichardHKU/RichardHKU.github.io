---
permalink: /
title: "Biography"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am Wenyong Zhou (周文涌), and I earned my PhD from The University of Hong Kong (HKU), where I had the privilege of being guided by the inspiring mentorship of Prof. Ngai Wong and Prof. Can Li as a proud member of the vibrant Next Gen AI (NGai) Lab family. My academic journey began with a Bachelor’s degree in 2019 from the School of Microelectronics at Tianjin University, where I was fortunate to work under the guidance of Prof. Yugong Wu, whose support sparked my curiosity for research. I later pursued my Master’s degree in Electrical and Computer Engineering at the McCormick School of Engineering, Northwestern University, where I was deeply influenced by the visionary mentorship of Prof. Seda Ogrenci.

My research focuses on implicit neural representations (INRs), exploring their potential for efficient data representation and processing. I am also working on developing next-generation computing platforms, with a particular focus on compute-in-memory (CIM) technologies to enhance efficiency. Additionally, I am passionate about optimizing Large Language Models (LLMs) through techniques such as quantization, pruning, and knowledge distillation to improve their performance and scalability.

Currently, I am working at Zhicun (Witmem) Technology, starting from November 2024, focusing on low-bit training of LLMs using cutting-edge analog CIM technology. This research aims to unlock the potential of low-bit training, enabling more energy-efficient, faster, and scalable LLM deployment on advanced hardware.


Research Interests
======
- INRs are an innovative technique for encoding data such as images, signals, and 3D scenes in a continuous and compact form using neural networks. Unlike traditional discrete representations, INRs model data as continuous functions, offering greater detail and adaptability. By encoding information within a compact neural network, they reduce the need for extensive storage while maintaining high-fidelity representations. This makes them particularly valuable in applications where storage and computational resources are limited.
- CIM integrates memory and computation into a single unit, significantly reducing data movement—one of the primary bottlenecks in traditional systems—and boosting both energy efficiency and computational speed. This approach is especially effective for accelerating AI workloads, enabling faster and more efficient processing of complex algorithms while addressing the growing demand for energy-efficient computing solutions.
- LLMs have become a cornerstone of modern AI, driving advancements in applications ranging from natural language understanding to content generation. Efficient LLMs utilize techniques such as quantization, pruning, and model compression to minimize their size and computational requirements without compromising performance. These optimizations allow LLMs to operate on smaller devices while maintaining high accuracy, democratizing access to advanced AI capabilities and broadening their range of applications.


Recent News
======
- 2025.08 - One paper was accepted by ASICON.
- 2025.06 - One paper was accepted by IEEE TCAS-II.
- 2025.03 - One paper was accepted by ICME 2025.
- 2025.01 - One paper was accepted by EDTM 2025.
- 2024.12 - Two papers were accepted by ICCASP 2025.
- 2024.11 - One paper was accepted by DATE 2025.

Selected Publications 
======
*: Equal contribution.
- INRs
  - **W. Zhou\***, B. Li\*, T. Wu, C. Ding, Z. Liu and N. Wong. QuadINR: Quadratic Implicit Neural Representations for Efficient Memristor-based CIM System, *IEEE Transactions on Circuits and Systems II: Express Briefs*.  
  - J. Ren\*, **W. Zhou\***, T. Wu, Y. Cheng, Z. Liu and N. Wong. Patch-Based Implicit Neural Representations for Efficient and Scalable Inference, *IEEE Transactions on Circuits and Systems II: Express Briefs*, (Major revision).
  - **W. Zhou\***, J. Ren\*, T. Wu, Y. Cheng, Z. Liu and N. Wong. Distribution-Aware Hadamard Quantization for Hardware-Efficient Implicit Neural Representations, *2025 IEEE International Conference on Multimedia and Expo (ICME), Nantes, France, 2025*.
  - **W. Zhou\***, T. Wu\*, Y. Cheng, C. Zhang, Z. Liu and N. Wong. MINR: Efficient Implicit Neural Representations for Multi-Image Encoding, *ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Hyderabad, India, 2025*.
  - **W. Zhou\***, Y. Cheng\*, T. Wu, C. Zhang, Z. Liu and N. Wong. Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations, *ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Hyderabad, India, 2025.
  
- Next-Generation Computing Platforms
  - **W. Zhou**, Y. Ren, Z. Liu and N. Wong. Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators, *IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems*.
  - T. Wu\*, C. Ding\*, **W. Zhou\***, Y. Cheng, X. Feng, C. Shi, Z. Liu and N. Wong. HaLoRA: Hardware-aware Low-Rank Adaptation for Large Language Models Based on Hybrid Compute-in-Memory Architecture, *IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems*, (under review).
  - **W. Zhou**, T. Wu, C. Ding, Y. Ren, Z. Liu and N. Wong. Towards RRAM-based Transformer-based Vision Models with Noise-aware Knowledge Distillation, *2025 Design, Automation & Test in Europe Conference & Exhibition (DATE), Lyon, France, 2025*.
  - **W. Zhou\***, Y. Ren\*, J. Zhou, C. Ding, Z. Liu, and N. Wong, RRAM-Based Isotropic CNNs with High Robustness and Resource Utilization Rate, *2025 9th IEEE Electron Devices Technology & Manufacturing Conference (EDTM), Hong Kong, China, 2025*.
  - **W. Zhou\***, Y. Ren\*, J. Zhou, T. Hou, and N. Wong, A Time- and Energy-Efficient CNN with Dense Connections on Memristor-Based Chips, *2023 IEEE 15th International Conference on ASIC (ASICON), Nanjing, China, 2023*.

- Efficient LLMs
  - On the way

More about me
------
- I enjoy staying active and challenging myself through sports like table tennis, badminton, and basketball. These activities not only keep me physically fit but also teach me the value of teamwork, strategy, and perseverance.
- I am deeply passionate about reading, particularly delving into history and politics through a financial lens. I find it fascinating to uncover how financial systems and economic decisions have shaped societies and historical events.
- Traveling is one of my greatest joys. It allows me to explore new cultures, meet diverse people, and experience the world from different perspectives. Each trip enriches my understanding of humanity and inspires new ways of thinking.

(Last updated on Aug., 2025)
